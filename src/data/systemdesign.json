{
  "systemDesignTopics": [
    {
      "id": "hld-1",
      "title": "HLD - Web Services",
      "summary": "High-level architectural designs for scalable web services and applications",
      "questions": [
        {
          "id": "hld_1",
          "title": "Design a URL Shortener",
          "routeName": "url_shortener",
          "difficulty": "Medium",
          "companies": [
            "Google",
            "Amazon",
            "Microsoft",
            "Apple",
            "Facebook",
            "Netflix"
          ],
          "tags": [
            "System Design",
            "Distributed Systems",
            "Hashing",
            "Database Design"
          ],
          "description": "Design a URL shortening service like bit.ly or TinyURL that converts long URLs into short, memorable links.",
          "detailedDescription": "A URL shortener maps long URLs to short aliases. The system should handle billions of URLs, provide fast redirection, support custom aliases, track analytics, and scale horizontally. Key considerations: unique short URL generation, handling collisions, database partitioning, caching strategy, and rate limiting.",
          "approach": "1. Use base62 encoding (a-z, A-Z, 0-9) for short URLs giving 62^7 = 3.5 trillion possibilities with 7 characters. 2. Generate short URLs using hash functions (MD5/SHA256) and take first 7 characters, or use auto-incrementing IDs converted to base62. 3. Store mappings in NoSQL database (Cassandra/DynamoDB) partitioned by hash of short URL. 4. Implement Redis/Memcached for caching popular URLs. 5. Use CDN for global distribution. 6. Rate limiting per user/IP to prevent abuse. 7. Analytics service to track clicks asynchronously.",
          "solution": "Architecture Components: 1) API Gateway for rate limiting and authentication. 2) Application servers (stateless) for URL creation and redirection. 3) Database cluster (master-slave replication) for storing URL mappings. 4) Cache layer (Redis cluster) with LRU eviction. 5) Analytics service with message queue for async processing. 6) CDN for serving redirects globally. Database Schema: URLs table (short_url PK, long_url, user_id, created_at, expires_at), Analytics table (short_url, timestamp, country, device, referrer). APIs: POST /shorten {longUrl, customAlias?, userId}, GET /{shortUrl} returns 302 redirect.",
          "code": "// Short URL Generation\nfunction generateShortUrl(longUrl, counter) {\n  const base62 = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ';\n  let shortUrl = '';\n  let num = counter;\n  while (num > 0) {\n    shortUrl = base62[num % 62] + shortUrl;\n    num = Math.floor(num / 62);\n  }\n  return shortUrl.padStart(7, '0');\n}\n\n// Redirection with caching\nasync function redirect(shortUrl) {\n  let longUrl = await cache.get(shortUrl);\n  if (!longUrl) {\n    longUrl = await db.query('SELECT long_url FROM urls WHERE short_url = ?', [shortUrl]);\n    await cache.set(shortUrl, longUrl, 3600);\n  }\n  await analyticsQueue.push({shortUrl, timestamp: Date.now()});\n  return longUrl;\n}",
          "timeComplexity": "O(1) for URL generation and redirection with caching. O(log n) for database lookups without cache.",
          "spaceComplexity": "O(n) where n is the number of URLs stored. Cache requires O(m) where m is number of hot URLs.",
          "keyInsights": "Use base62 encoding for compact URLs. Partition database by hash for horizontal scaling. Cache hot URLs aggressively (80-20 rule). Handle race conditions in custom alias creation with optimistic locking. Pre-generate short URLs in batches for faster creation. Use bloom filters to quickly check URL existence. Implement expiration for temporary URLs. Consider security: prevent enumeration attacks, validate URLs, implement CAPTCHA for high-volume users.",
          "link": "https://leetcode.com/discuss/interview-question/system-design/"
        },
        {
          "id": "hld_2",
          "title": "Design Instagram",
          "routeName": "instagram",
          "difficulty": "Hard",
          "companies": ["Meta", "Google", "Amazon", "Apple", "Netflix"],
          "tags": [
            "System Design",
            "Image Storage",
            "News Feed",
            "Social Network"
          ],
          "description": "Design a photo-sharing social media platform like Instagram with features for uploading photos, following users, and viewing feeds.",
          "detailedDescription": "Instagram allows users to upload photos/videos, follow other users, like/comment on posts, and view a personalized feed. System must handle billions of users, petabytes of media, real-time notifications, and serve content globally with low latency. Key challenges: media storage and delivery, feed generation, recommendation system, and handling celebrity accounts with millions of followers.",
          "approach": "1. Separate read and write paths for scalability. 2. Store metadata in sharded MySQL/PostgreSQL, media files in object storage (S3/GCS). 3. Use CDN for global media distribution. 4. Fan-out on write for feed generation with fallback to fan-out on read for celebrities. 5. Redis for caching feeds and user data. 6. ElasticSearch for search functionality. 7. Kafka for event streaming and notifications. 8. Graph database (Neo4j) for social graph queries.",
          "solution": "Core Services: 1) Upload Service: resizes images, generates thumbnails, uploads to S3, stores metadata in DB. 2) Feed Service: generates feed using fan-out approach, caches in Redis. 3) Timeline Service: retrieves posts chronologically or via recommendation algorithm. 4) Notification Service: push notifications via FCM/APNs. 5) Search Service: indexes users/hashtags in ElasticSearch. Database Design: Users (user_id, username, profile_pic, bio), Posts (post_id, user_id, image_urls, caption, created_at), Followers (follower_id, followee_id), Likes/Comments tables. Feed Generation: For normal users, fan-out on write (push posts to all followers' feeds). For celebrities, fan-out on read (fetch posts when user opens app). Use ML ranking for feed ordering.",
          "code": "// Feed generation (hybrid approach)\nasync function generateFeed(userId, limit = 50) {\n  const followedUsers = await getFollowing(userId);\n  const [normalUsers, celebrities] = partition(followedUsers, \n    user => user.followerCount < 1000000);\n  \n  // Get pre-computed feed for normal users\n  let feed = await redis.zrevrange(`feed:${userId}`, 0, limit);\n  \n  // Fetch recent posts from celebrities on-demand\n  if (celebrities.length > 0) {\n    const celebrityPosts = await db.query(\n      'SELECT * FROM posts WHERE user_id IN (?) AND created_at > ? ORDER BY created_at DESC LIMIT 20',\n      [celebrities.map(c => c.id), Date.now() - 24*3600*1000]\n    );\n    feed = mergeSorted(feed, celebrityPosts);\n  }\n  \n  // Apply ML ranking\n  return rankPosts(feed, userId).slice(0, limit);\n}",
          "timeComplexity": "Feed generation: O(n log n) where n is number of posts. Write: O(f) where f is follower count for fan-out.",
          "spaceComplexity": "O(u * p) where u is users and p is average posts per feed. Media storage: O(total images * avg size).",
          "keyInsights": "Separate hot and cold storage for media (recent in SSD, old in HDD/Glacier). Use image CDN with multiple formats (WebP, JPEG) for optimization. Implement progressive JPEG for faster perceived load times. Fan-out on write for < 1M followers, fan-out on read for celebrities. Cache feed for 15-30 minutes. Use consistent hashing for feed cache distribution. Store denormalized data for faster reads. Implement rate limiting on upload/follow actions. Use event sourcing for audit trail.",
          "link": "https://www.youtube.com/watch?v=QmX2NPkJTKg"
        },
        {
          "id": "hld_3",
          "title": "Design Twitter",
          "routeName": "twitter",
          "difficulty": "Hard",
          "companies": ["Twitter", "Meta", "Google", "Amazon", "Apple"],
          "tags": ["System Design", "Timeline", "Social Network", "Real-time"],
          "description": "Design a microblogging platform like Twitter with tweet posting, following, timeline, and trending topics.",
          "detailedDescription": "Twitter enables users to post 280-character tweets, follow others, retweet, like, and view home/user timelines. System must handle 500M+ users, 6000 tweets/sec, real-time updates, trending topics calculation, and search. Key challenges: timeline generation at scale, handling viral tweets, real-time notifications, and spam detection.",
          "approach": "1. Use fan-out service for timeline generation with hybrid push-pull model. 2. Redis for timeline cache and user graph. 3. Cassandra for tweet storage (optimized for writes). 4. Storm/Flink for real-time trending calculation. 5. ElasticSearch for search. 6. Kafka for event streaming. 7. ML models for spam detection and recommendation. 8. WebSocket connections for real-time updates.",
          "solution": "Microservices: 1) Tweet Service: handles tweet creation, stores in Cassandra partitioned by tweet_id. 2) Timeline Service: generates home timeline (tweets from followed users) and user timeline (user's own tweets). 3) Fanout Service: asynchronously pushes tweets to followers' timelines in Redis. 4) Trending Service: processes tweet stream, counts hashtags/keywords in sliding windows, ranks trends. 5) Search Service: indexes tweets in ElasticSearch. 6) Notification Service: WebSocket server for real-time updates. Data Model: Tweets (id, user_id, text, media_urls, timestamp, retweet_count, like_count), Users (id, username, follower_count, following_count), Social Graph in Redis. Timeline: store tweet IDs in Redis sorted sets sorted by timestamp.",
          "code": "// Fanout service with optimization\nasync function fanoutTweet(tweet) {\n  const followers = await getFollowers(tweet.userId);\n  \n  // Partition followers into online and offline\n  const onlineFollowers = await redis.smembers(`online_users`);\n  const onlineSet = new Set(onlineFollowers);\n  \n  // Push to online users immediately via WebSocket\n  const onlineBatch = followers.filter(f => onlineSet.has(f));\n  await Promise.all(onlineBatch.map(async follower => {\n    await redis.zadd(`timeline:${follower}`, tweet.timestamp, tweet.id);\n    await websocket.send(follower, {type: 'new_tweet', tweet});\n  }));\n  \n  // Queue offline users for async processing\n  const offlineBatch = followers.filter(f => !onlineSet.has(f));\n  await queue.pushBatch(offlineBatch.map(f => ({follower: f, tweetId: tweet.id})));\n  \n  // Trim timelines to keep only recent 800 tweets\n  await redis.zremrangebyrank(`timeline:*`, 0, -801);\n}",
          "timeComplexity": "Tweet posting: O(1) write to DB, O(f) fanout where f is followers. Timeline fetch: O(1) from cache.",
          "spaceComplexity": "O(u * t) where u is users and t is tweets per timeline (typically limited to 800).",
          "keyInsights": "Use push model for active users, pull for inactive. Implement celebrity detection (> 10M followers) with separate handling. Cache timelines in Redis with 24hr TTL. Use bloom filters to detect spam. Implement rate limiting: 300 tweets/3 hours per user. Trending algorithm: count hashtags in 15-min windows, boost recent trends, personalize by location. Store tweets in time-ordered UUID for efficient range queries. Use message queue for async fanout to handle spikes. Implement circuit breakers for timeline generation failures.",
          "link": "https://github.com/donnemartin/system-design-primer"
        },
        {
          "id": "hld_4",
          "title": "Design YouTube",
          "routeName": "youtube",
          "difficulty": "Hard",
          "companies": ["Google", "Netflix", "Amazon", "Meta", "Apple"],
          "tags": [
            "System Design",
            "Video Streaming",
            "CDN",
            "Distributed Storage"
          ],
          "description": "Design a video sharing and streaming platform like YouTube that handles video uploads, processing, storage, and global delivery.",
          "detailedDescription": "YouTube allows users to upload videos, stream content, search, comment, and discover recommendations. System must handle 500+ hours of video uploaded per minute, billions of daily views, multiple resolutions/formats, live streaming, and global CDN delivery. Key challenges: video encoding pipeline, storage optimization, bandwidth costs, recommendation system, and copyright detection.",
          "approach": "1. Chunked upload to S3/GCS with multipart upload. 2. Video processing pipeline: transcoding to multiple resolutions (1080p, 720p, 480p, 360p), formats (H.264, VP9, AV1), adaptive bitrate streaming (HLS/DASH). 3. Store processed videos in distributed object storage with tiering (hot/warm/cold). 4. CDN for global delivery with edge caching. 5. Separate metadata DB (MySQL) and search index (ElasticSearch). 6. Recommendation engine using collaborative filtering and deep learning. 7. Distributed message queue for async processing.",
          "solution": "Components: 1) Upload Service: handles chunked uploads, stores raw video in S3, queues for processing. 2) Transcoding Service: distributed workers fetch from queue, transcode using FFmpeg, generate thumbnails, extract audio. 3) Storage: raw videos in S3 Glacier, processed videos in S3 Standard/IA with CDN. 4) Streaming Service: serves video chunks via CDN, handles adaptive bitrate switching. 5) Metadata Service: stores video info (title, description, views, likes) in sharded MySQL. 6) Search Service: ElasticSearch with video metadata, transcripts, tags. 7) Recommendation Service: collaborative filtering + content-based + deep learning (watch history, likes, search). 8) Analytics: track watch time, engagement in data warehouse.",
          "code": "// Adaptive bitrate video serving\nfunction getVideoManifest(videoId, userId) {\n  const qualities = ['1080p', '720p', '480p', '360p', '240p'];\n  const userBandwidth = estimateBandwidth(userId);\n  \n  // Generate HLS manifest\n  const manifest = {\n    version: 3,\n    targetDuration: 10,\n    mediaSequence: 0,\n    playlists: qualities.map(q => ({\n      resolution: q,\n      bandwidth: getBandwidthForQuality(q),\n      url: `${CDN_URL}/${videoId}/${q}/playlist.m3u8`\n    }))\n  };\n  \n  // Start with quality matching user bandwidth\n  manifest.defaultQuality = qualities.find(q => \n    getBandwidthForQuality(q) <= userBandwidth * 0.8\n  ) || '360p';\n  \n  return manifest;\n}\n\n// Recommendation system\nasync function getRecommendations(userId, limit = 20) {\n  const userHistory = await getUserWatchHistory(userId);\n  const userPreferences = await extractPreferences(userHistory);\n  \n  // Hybrid approach: collaborative + content-based\n  const collaborative = await getCollaborativeFiltering(userId);\n  const contentBased = await getContentBasedRecs(userPreferences);\n  const trending = await getTrendingVideos();\n  \n  // Merge and rank\n  const combined = [\n    ...collaborative.map(v => ({...v, score: v.score * 0.5})),\n    ...contentBased.map(v => ({...v, score: v.score * 0.3})),\n    ...trending.map(v => ({...v, score: v.score * 0.2}))\n  ];\n  \n  return combined.sort((a, b) => b.score - a.score).slice(0, limit);\n}",
          "timeComplexity": "Video upload: O(n) where n is video size. Streaming: O(1) per chunk. Transcoding: O(n * m) where m is number of formats.",
          "spaceComplexity": "O(v * f * r) where v is videos, f is formats, r is resolutions. Typically 10-20x original size after transcoding.",
          "keyInsights": "Use keyframe-aligned chunking for smooth seeking. Implement 3-tier storage: SSD for recent/popular, HDD for regular, Glacier for old. Pre-generate thumbnails at multiple timestamps. Use codec ladder optimization (different bitrates for different resolutions). Implement CDN with 100+ POPs globally. Cache popular videos at edge. Use distributed transcoding: split video into segments, process in parallel. Implement content ID system for copyright detection (fingerprinting). Store metadata in denormalized form for faster queries. Use view count approximation (update every 100 views) to reduce DB writes.",
          "link": "https://www.youtube.com/watch?v=jPKTo1iGQiE"
        },
        {
          "id": "hld_5",
          "title": "Design Netflix",
          "routeName": "netflix",
          "difficulty": "Hard",
          "companies": ["Netflix", "Amazon", "Google", "Apple", "Disney"],
          "tags": ["System Design", "Video Streaming", "CDN", "Recommendation"],
          "description": "Design a video streaming service like Netflix with content delivery, user profiles, recommendations, and offline viewing.",
          "detailedDescription": "Netflix streams video content to 200M+ subscribers globally with minimal buffering, personalized recommendations, multiple profiles per account, offline downloads, and 4K HDR support. Key challenges: global content delivery, reducing bandwidth costs, recommendation accuracy, DRM, and handling regional content licensing.",
          "approach": "1. Own CDN (Open Connect) with appliances in ISP networks. 2. Adaptive bitrate streaming with per-title encoding optimization. 3. Pre-encoding during off-peak hours. 4. Microservices architecture on AWS. 5. Cassandra for distributed data storage. 6. Recommendation engine using deep learning (matrix factorization, RNN). 7. A/B testing framework for every feature. 8. Chaos engineering for resilience.",
          "solution": "Architecture: 1) Control Plane (AWS): user management, billing, recommendation, encoding jobs. 2) Data Plane (Open Connect CDN): video delivery from edge servers. 3) Encoding: per-title optimization (analyze each video, determine optimal bitrate ladder), store multiple versions. 4) Client: adaptive player downloads chunks, switches quality based on bandwidth. 5) Recommendation: collaborative filtering (user-user, item-item similarity), deep neural networks (user history, context), bandits for exploration. 6) Search: ElasticSearch with NLP for fuzzy matching. 7) Playback: track watch position, resume across devices. Database: Cassandra for user data, watch history. EVCache (Memcached) for caching. MySQL for billing.",
          "code": "// Per-title encoding optimization\nfunction optimizeEncodingLadder(videoMetadata) {\n  const complexity = analyzeComplexity(videoMetadata); // scene changes, motion\n  \n  // High complexity videos need higher bitrates\n  if (complexity > 0.8) {\n    return {\n      '2160p': '25Mbps',\n      '1080p': '8Mbps',\n      '720p': '5Mbps',\n      '480p': '2.5Mbps'\n    };\n  } else if (complexity > 0.5) {\n    return {\n      '2160p': '20Mbps',\n      '1080p': '5Mbps',\n      '720p': '3Mbps',\n      '480p': '1.5Mbps'\n    };\n  } else {\n    // Low complexity (animation, slow scenes)\n    return {\n      '2160p': '16Mbps',\n      '1080p': '3Mbps',\n      '720p': '2Mbps',\n      '480p': '1Mbps'\n    };\n  }\n}\n\n// Recommendation system\nclass RecommendationEngine {\n  async getPersonalizedRows(userId) {\n    const profile = await getUserProfile(userId);\n    const watchHistory = await getWatchHistory(userId);\n    \n    // Generate multiple carousels\n    const rows = [\n      await this.getContinueWatching(userId),\n      await this.getTrendingNow(profile.country),\n      await this.getBecauseYouWatched(watchHistory[0]),\n      await this.getTopPicksForUser(userId),\n      await this.getNewReleases(profile.preferences),\n      await this.getGenreRow('Action', userId),\n      await this.getAwardWinning()\n    ];\n    \n    // Personalize order of rows based on user behavior\n    return this.rankRows(rows, profile);\n  }\n}",
          "timeComplexity": "Streaming: O(1) per chunk from edge cache. Recommendations: O(n log n) where n is catalog size, computed offline.",
          "spaceComplexity": "O(v * e * r) where v is videos, e is encodings per title (3-5), r is resolutions. Petabytes across CDN.",
          "keyInsights": "Place CDN servers inside ISP networks to reduce transit costs (Open Connect program). Pre-position content on edge servers during off-peak using predictive algorithms. Use per-title encoding to save 20-30% bandwidth. Implement predictive caching based on viewing patterns. Use multiple CDNs as fallback. Encode at night when compute is cheaper. Recommendation accounts for 80% of viewing. Use bandits to balance exploitation (show similar content) vs exploration (discover new preferences). Implement sophisticated A/B testing (interleaving, multi-armed bandits). Use chaos engineering (Chaos Monkey) to ensure resilience.",
          "link": "https://netflixtechblog.com/"
        },
        {
          "id": "hld_6",
          "title": "Design WhatsApp",
          "routeName": "whatsapp",
          "difficulty": "Hard",
          "companies": ["Meta", "Google", "Microsoft", "Apple", "Telegram"],
          "tags": [
            "System Design",
            "Real-time Messaging",
            "WebSocket",
            "End-to-End Encryption"
          ],
          "description": "Design a real-time messaging platform like WhatsApp with text/media messaging, group chats, end-to-end encryption, and delivery receipts.",
          "detailedDescription": "WhatsApp handles 100B+ messages daily with real-time delivery, end-to-end encryption, read receipts, typing indicators, group chats (256 members), media sharing, voice/video calls, and status updates. Key challenges: message delivery guarantees, encryption at scale, handling offline users, message sync across devices, and maintaining low latency globally.",
          "approach": "1. WebSocket for persistent connections. 2. Message queue for async delivery. 3. Signal Protocol for end-to-end encryption. 4. Cassandra for message storage. 5. Redis for session management and online presence. 6. Media stored in S3 with CDN. 7. XMPP-based protocol. 8. Master-master replication across datacenters. 9. Erlang for high concurrency.",
          "solution": "Components: 1) Connection Service: maintains WebSocket connections, routes messages. 2) Message Service: stores messages in Cassandra, manages delivery. 3) Group Service: manages group metadata, message fanout. 4) Media Service: handles image/video/audio upload to S3. 5) Presence Service: tracks online status in Redis. 6) Notification Service: sends push notifications to offline users. 7) Encryption: Signal Protocol (Double Ratchet) for E2E encryption, each message has unique key. Message Flow: sender -> connection server -> message queue -> connection server -> receiver. For offline users, store in DB and deliver when online. Delivery receipts: sent, delivered (reached server), read (opened). Group messages: send individual encrypted copy to each member.",
          "code": "// Message delivery system\nclass MessageDelivery {\n  async sendMessage(senderId, recipientId, content) {\n    // Encrypt message using recipient's public key\n    const encryptedContent = await encryptE2E(content, recipientId);\n    \n    const message = {\n      id: generateUUID(),\n      senderId,\n      recipientId,\n      content: encryptedContent,\n      timestamp: Date.now(),\n      status: 'sent'\n    };\n    \n    // Store in database\n    await cassandra.insert('messages', message);\n    \n    // Try to deliver immediately if user is online\n    const isOnline = await redis.sismember('online_users', recipientId);\n    if (isOnline) {\n      const connection = await getWebSocketConnection(recipientId);\n      await connection.send(JSON.stringify(message));\n      message.status = 'delivered';\n      await cassandra.update('messages', message.id, {status: 'delivered'});\n    } else {\n      // Queue for delivery when user comes online\n      await redis.lpush(`pending:${recipientId}`, message.id);\n      // Send push notification\n      await sendPushNotification(recipientId, {\n        title: getSenderName(senderId),\n        body: 'New message'\n      });\n    }\n    \n    return message;\n  }\n  \n  async onUserOnline(userId) {\n    // Deliver pending messages\n    const pendingIds = await redis.lrange(`pending:${userId}`, 0, -1);\n    const messages = await cassandra.getMessages(pendingIds);\n    \n    const connection = await getWebSocketConnection(userId);\n    for (const msg of messages) {\n      await connection.send(JSON.stringify(msg));\n    }\n    \n    await redis.del(`pending:${userId}`);\n  }\n}\n\n// Group message handling\nasync function sendGroupMessage(senderId, groupId, content) {\n  const members = await getGroupMembers(groupId);\n  \n  // Send individual encrypted message to each member\n  await Promise.all(members.map(async (member) => {\n    if (member.id === senderId) return;\n    \n    // Each message encrypted with member's public key\n    const encryptedContent = await encryptE2E(content, member.id);\n    await sendMessage(senderId, member.id, encryptedContent);\n  }));\n}",
          "timeComplexity": "Message send: O(1) for 1-1, O(n) for groups where n is members. Retrieval: O(1) with proper indexing.",
          "spaceComplexity": "O(m) where m is total messages. Media stored separately. Messages older than 30 days moved to cold storage.",
          "keyInsights": "Use long polling or WebSocket for real-time delivery. Implement exponential backoff for offline message delivery. Store messages on server for cloud backup but maintain E2E encryption. Use sequence numbers per conversation to detect missing messages. Implement message compression (gzip). For groups, send individual encrypted copy to each member (no shared group key for security). Use Merkle trees for efficient message sync across devices. Implement typing indicators with debouncing. Store media as encrypted blobs. Use connection pooling for DB. Implement geographically distributed datacenters with master-master replication. Handle split-brain with vector clocks.",
          "link": "https://www.whatsapp.com/security/"
        }
      ]
    },
    {
      "id": "hld-2",
      "title": "HLD - Distributed Systems",
      "summary": "High-level designs for distributed systems, data storage, and infrastructure",
      "questions": [
        {
          "id": "hld_7",
          "title": "Design a Distributed Cache",
          "routeName": "distributed_cache",
          "difficulty": "Hard",
          "companies": ["Google", "Amazon", "Microsoft", "Meta", "Netflix"],
          "tags": [
            "System Design",
            "Caching",
            "Distributed Systems",
            "Consistent Hashing"
          ],
          "description": "Design a distributed caching system like Memcached or Redis that provides fast key-value storage across multiple nodes.",
          "detailedDescription": "A distributed cache stores frequently accessed data in-memory across multiple nodes to reduce database load and improve response times. System must handle node failures, data distribution, cache invalidation, hot keys, and provide high availability. Use cases: session storage, database query caching, API response caching, rate limiting counters.",
          "approach": "1. Consistent hashing for data distribution to minimize reorganization on node add/remove. 2. Replication factor of 3 for availability. 3. LRU/LFU eviction policy when memory full. 4. Client-side or proxy-based sharding. 5. Write-through or write-back strategy. 6. Gossip protocol for membership and failure detection. 7. Read repair for consistency. 8. Monitoring for hit rate, latency, hot keys.",
          "solution": "Architecture: 1) Cache Nodes: in-memory storage with hash map, stores key-value pairs. 2) Consistent Hash Ring: distributes keys across nodes, uses virtual nodes (vnodes) for balance. 3) Client Library: handles hashing, routing, retries, connection pooling. 4) Replication: async replication to N-1 replica nodes. 5) Eviction: background thread runs LRU eviction when memory > 90%. 6) Monitoring: track hit rate, memory usage, request latency per node. Data Structure: Hash map for O(1) lookups, doubly linked list for LRU ordering. Operations: GET key -> hash to node -> retrieve. SET key val ttl -> hash to node -> store with expiry. DELETE key -> hash to node -> remove. Consistency: eventual consistency with last-write-wins.",
          "code": "// Consistent hashing implementation\nclass ConsistentHash {\n  constructor(nodes, virtualNodes = 150) {\n    this.ring = new Map();\n    this.sortedKeys = [];\n    this.virtualNodes = virtualNodes;\n    nodes.forEach(node => this.addNode(node));\n  }\n  \n  addNode(node) {\n    for (let i = 0; i < this.virtualNodes; i++) {\n      const hash = this.hash(`${node.id}:${i}`);\n      this.ring.set(hash, node);\n    }\n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n  \n  removeNode(node) {\n    for (let i = 0; i < this.virtualNodes; i++) {\n      const hash = this.hash(`${node.id}:${i}`);\n      this.ring.delete(hash);\n    }\n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n  \n  getNode(key) {\n    if (this.ring.size === 0) return null;\n    const hash = this.hash(key);\n    // Binary search for first node >= hash\n    const idx = this.sortedKeys.findIndex(k => k >= hash);\n    const nodeHash = idx === -1 ? this.sortedKeys[0] : this.sortedKeys[idx];\n    return this.ring.get(nodeHash);\n  }\n  \n  hash(key) {\n    // Use MurmurHash or similar\n    let hash = 0;\n    for (let i = 0; i < key.length; i++) {\n      hash = ((hash << 5) - hash) + key.charCodeAt(i);\n      hash = hash & hash;\n    }\n    return Math.abs(hash);\n  }\n}\n// LRU Cache implementation\nclass LRUCache {\n  constructor(capacity) {\n    this.capacity = capacity;\n    this.cache = new Map();\n    this.head = { key: null, value: null, prev: null, next: null };\n    this.tail = { key: null, value: null, prev: null, next: null };\n    this.head.next = this.tail;\n    this.tail.prev = this.head;\n  }\n  \n  get(key) {\n    if (!this.cache.has(key)) return null;\n    const node = this.cache.get(key);\n    \n    // Check TTL\n    if (node.expiry && Date.now() > node.expiry) {\n      this.delete(key);\n      return null;\n    }\n    \n    this.moveToHead(node);\n    return node.value;\n  }\n  \n  set(key, value, ttl = null) {\n    if (this.cache.has(key)) {\n      const node = this.cache.get(key);\n      node.value = value;\n      node.expiry = ttl ? Date.now() + ttl * 1000 : null;\n      this.moveToHead(node);\n    } else {\n      if (this.cache.size >= this.capacity) {\n        this.evict();\n      }\n      const node = {\n        key,\n        value,\n        expiry: ttl ? Date.now() + ttl * 1000 : null,\n        prev: null,\n        next: null\n      };\n      this.cache.set(key, node);\n      this.addToHead(node);\n    }\n  }\n  \n  evict() {\n    const lru = this.tail.prev;\n    this.removeNode(lru);\n    this.cache.delete(lru.key);\n  }\n}",
          "timeComplexity": "GET/SET/DELETE: O(1) average case with hash table. Node lookup with consistent hashing: O(log n) with binary search.",
          "spaceComplexity": "O(n) where n is number of cached items. Virtual nodes add O(k * m) where k is nodes and m is virtual nodes per physical node.",
          "keyInsights": "Use consistent hashing with virtual nodes (100-200 per physical node) for even distribution. Implement read-through and write-through patterns. Cache aside pattern for flexibility. Use connection pooling to reduce overhead. Implement circuit breakers for failed nodes. Monitor cache hit rate (target >80%). Use lazy expiration (check TTL on access) + background cleanup. Handle thundering herd with lock or bloom filter. Implement cache warming for predictable loads. Use probabilistic data structures (Bloom filters, Count-Min Sketch) for space efficiency. Partition hot keys across multiple cache entries.",
          "link": "https://github.com/memcached/memcached"
        },
        {
          "id": "hld_8",
          "title": "Design Uber",
          "routeName": "uber",
          "difficulty": "Hard",
          "companies": ["Uber", "Lyft", "Google", "Amazon", "Apple"],
          "tags": [
            "System Design",
            "Geospatial",
            "Real-time Matching",
            "Location Services"
          ],
          "description": "Design a ride-sharing platform like Uber with real-time driver matching, location tracking, pricing, and payments.",
          "detailedDescription": "Uber connects riders with nearby drivers in real-time. System must handle location updates from millions of drivers, match riders to optimal drivers within seconds, calculate dynamic pricing, process payments, track rides, and provide ETAs. Key challenges: geospatial indexing, real-time matching at scale, handling surge pricing, fraud detection, and ensuring reliability for mission-critical trips.",
          "approach": "1. Geospatial indexing using QuadTree or Google S2 for proximity search. 2. Real-time location updates via WebSocket. 3. Matching algorithm considering distance, rating, driver preferences. 4. Redis for fast driver location lookups. 5. Kafka for event streaming. 6. PostgreSQL with PostGIS for persistent storage. 7. Surge pricing based on supply-demand ratio. 8. Microservices for trip, payment, notification, routing.",
          "solution": "Core Services: 1) Location Service: receives GPS updates from drivers every 4 seconds, stores in Redis with geospatial index, updates QuadTree. 2) Matching Service: when rider requests, queries nearby drivers from QuadTree (radius search), ranks by ETA/rating, sends request to top 5, first to accept gets trip. 3) Trip Service: manages trip lifecycle (requested, matched, started, completed), stores in PostgreSQL. 4) Pricing Service: calculates fare using base + distance + time + surge multiplier. 5) Payment Service: processes payments via Stripe/Braintree. 6) ETA Service: uses historical traffic data + real-time routing (Google Maps API) to predict arrival time. 7) Notification Service: sends push notifications for trip updates. QuadTree: divide world into grid, recursively subdivide busy cells, store driver IDs in leaf nodes.",
          "code": "// QuadTree for geospatial indexing\nclass QuadTree {\n  constructor(boundary, capacity = 50) {\n    this.boundary = boundary; // {minLat, maxLat, minLng, maxLng}\n    this.capacity = capacity;\n    this.drivers = [];\n    this.divided = false;\n    this.children = [null, null, null, null]; // NW, NE, SW, SE\n  }\n  \n  insert(driver) {\n    if (!this.boundary.contains(driver.lat, driver.lng)) return false;\n    \n    if (this.drivers.length < this.capacity) {\n      this.drivers.push(driver);\n      return true;\n    }\n    \n    if (!this.divided) this.subdivide();\n    \n    return this.children[0].insert(driver) ||\n           this.children[1].insert(driver) ||\n           this.children[2].insert(driver) ||\n           this.children[3].insert(driver);\n  }\n  \n  queryRadius(lat, lng, radius, found = []) {\n    if (!this.boundary.intersectsCircle(lat, lng, radius)) return found;\n    \n    for (const driver of this.drivers) {\n      const dist = haversineDistance(lat, lng, driver.lat, driver.lng);\n      if (dist <= radius) found.push({driver, distance: dist});\n    }\n    \n    if (this.divided) {\n      this.children.forEach(child => child.queryRadius(lat, lng, radius, found));\n    }\n    \n    return found;\n  }\n}\n\n// Driver matching algorithm\nasync function matchDriver(riderId, pickupLat, pickupLng) {\n  // Find nearby drivers (within 5km)\n  const nearbyDrivers = quadTree.queryRadius(pickupLat, pickupLng, 5000);\n  \n  // Filter available drivers\n  const available = nearbyDrivers.filter(d => \n    d.driver.status === 'available' && \n    d.driver.rating >= 4.5\n  );\n  \n  if (available.length === 0) return null;\n  \n  // Calculate ETA for each driver\n  const withETA = await Promise.all(available.map(async d => ({\n    ...d,\n    eta: await calculateETA(d.driver.lat, d.driver.lng, pickupLat, pickupLng)\n  })));\n  \n  // Rank by ETA and rating\n  withETA.sort((a, b) => {\n    const etaDiff = a.eta - b.eta;\n    if (Math.abs(etaDiff) < 60) { // Within 1 min, prefer higher rating\n      return b.driver.rating - a.driver.rating;\n    }\n    return etaDiff;\n  });\n  \n  // Send request to top 5 drivers\n  const topDrivers = withETA.slice(0, 5);\n  const acceptedDriver = await Promise.race(\n    topDrivers.map(d => sendRideRequest(d.driver.id, riderId))\n  );\n  \n  return acceptedDriver;\n}\n\n// Surge pricing calculation\nfunction calculateSurge(lat, lng) {\n  const region = getRegion(lat, lng);\n  const demand = getActiveRiders(region);\n  const supply = getAvailableDrivers(region);\n  const ratio = demand / Math.max(supply, 1);\n  \n  if (ratio < 1) return 1.0;\n  if (ratio < 2) return 1.5;\n  if (ratio < 3) return 2.0;\n  if (ratio < 5) return 2.5;\n  return 3.0; // Max surge\n}",
          "timeComplexity": "Driver insert/update in QuadTree: O(log n). Proximity search: O(log n + k) where k is results. Matching: O(m log m) where m is nearby drivers.",
          "spaceComplexity": "O(d) where d is active drivers. QuadTree uses O(d) space. Redis geospatial index: O(d).",
          "keyInsights": "Use Google S2 or H3 for better geospatial indexing (handles poles, consistent cell sizes). Update driver locations every 4 seconds (balance battery vs accuracy). Use Redis GEOADD/GEORADIUS for fast proximity queries. Implement predictive positioning (pre-move drivers to high-demand areas). Cache frequent routes for ETA calculation. Use websockets for real-time updates. Implement timeout for driver acceptance (15 seconds). Handle edge cases: driver cancellation, no drivers available, payment failures. Use event sourcing for trip state management. Implement fraud detection: impossible speeds, fake GPS, payment fraud. Calculate surge per geographic region every 5 minutes. Store trip history for ML-based demand prediction.",
          "link": "https://eng.uber.com/tech-stack-part-one/"
        },
        {
          "id": "hld_9",
          "title": "Design Dropbox",
          "routeName": "dropbox",
          "difficulty": "Hard",
          "companies": ["Dropbox", "Google", "Microsoft", "Amazon", "Apple"],
          "tags": [
            "System Design",
            "File Storage",
            "Synchronization",
            "Distributed Systems"
          ],
          "description": "Design a cloud file storage and synchronization service like Dropbox that enables users to store, sync, and share files across devices.",
          "detailedDescription": "Dropbox allows users to upload files, sync across devices, share with others, access file history, and work offline. System must handle petabytes of data, millions of concurrent users, efficient bandwidth usage, conflict resolution, and provide high availability. Key challenges: efficient sync algorithm, deduplication, handling large files, conflict resolution, and minimizing data transfer.",
          "approach": "1. Chunking: split files into 4MB blocks for efficient sync. 2. Content-based hashing (SHA-256) for deduplication. 3. Metadata service (MySQL) for file hierarchy, sharing, versions. 4. Block storage (S3) for actual file chunks. 5. Client-side sync algorithm with delta sync. 6. Message queue for sync notifications. 7. CDN for download acceleration. 8. Versioning with retention policy.",
          "solution": "Components: 1) Client Application: monitors file changes, chunks files, computes hashes, uploads only modified chunks, downloads changes. 2) Sync Service: receives file updates, broadcasts to connected clients via long polling/WebSocket. 3) Metadata Service: stores file/folder structure, permissions, versions in MySQL. 4) Block Storage: stores unique file chunks in S3, referenced by hash. 5) Notification Service: notifies clients of remote changes. 6) Sharing Service: manages shared folders, permissions. Sync Algorithm: 1) Client computes chunks and hashes. 2) Sends metadata to server. 3) Server identifies existing chunks (dedup). 4) Client uploads only new chunks. 5) Server updates metadata, notifies other clients. 6) Other clients download only changed chunks. Conflict Resolution: last-write-wins or create conflicted copy.",
          "code": "// File chunking and deduplication\nclass FileChunker {\n  constructor(chunkSize = 4 * 1024 * 1024) {\n    this.chunkSize = chunkSize;\n  }\n  \n  async chunkFile(file) {\n    const chunks = [];\n    let offset = 0;\n    \n    while (offset < file.size) {\n      const chunk = file.slice(offset, offset + this.chunkSize);\n      const buffer = await chunk.arrayBuffer();\n      const hash = await this.computeHash(buffer);\n      \n      chunks.push({\n        offset,\n        size: buffer.byteLength,\n        hash,\n        data: buffer\n      });\n      \n      offset += this.chunkSize;\n    }\n    \n    return chunks;\n  }\n  \n  async computeHash(buffer) {\n    const hashBuffer = await crypto.subtle.digest('SHA-256', buffer);\n    return Array.from(new Uint8Array(hashBuffer))\n      .map(b => b.toString(16).padStart(2, '0'))\n      .join('');\n  }\n}\n\n// Sync algorithm\nclass SyncEngine {\n  async syncFile(filePath, localFile) {\n    // 1. Chunk the file\n    const chunks = await fileChunker.chunkFile(localFile);\n    \n    // 2. Get file metadata from server\n    const serverMetadata = await api.getFileMetadata(filePath);\n    \n    // 3. Identify chunks to upload (delta sync)\n    const serverHashes = new Set(serverMetadata?.chunks.map(c => c.hash) || []);\n    const newChunks = chunks.filter(c => !serverHashes.has(c.hash));\n    \n    // 4. Upload only new chunks\n    await Promise.all(newChunks.map(chunk => \n      api.uploadChunk(chunk.hash, chunk.data)\n    ));\n    \n    // 5. Update metadata on server\n    await api.updateFileMetadata(filePath, {\n      chunks: chunks.map(c => ({ hash: c.hash, offset: c.offset, size: c.size })),\n      size: localFile.size,\n      modified: Date.now()\n    });\n    \n    console.log(`Uploaded ${newChunks.length}/${chunks.length} chunks`);\n  }\n  \n  async downloadFile(filePath, localPath) {\n    // 1. Get metadata\n    const metadata = await api.getFileMetadata(filePath);\n    \n    // 2. Download missing chunks\n    const localChunks = await this.getLocalChunks(localPath);\n    const localHashes = new Set(localChunks.map(c => c.hash));\n    const missingChunks = metadata.chunks.filter(c => !localHashes.has(c.hash));\n    \n    // 3. Download in parallel\n    const chunkData = await Promise.all(\n      missingChunks.map(c => api.downloadChunk(c.hash))\n    );\n    \n    // 4. Assemble file\n    await this.assembleFile(localPath, metadata.chunks, chunkData);\n  }\n  \n  async detectConflict(filePath, localModified, serverModified) {\n    if (Math.abs(localModified - serverModified) < 1000) {\n      // Modified within 1 second - use last-write-wins\n      return localModified > serverModified ? 'local' : 'server';\n    }\n    \n    // Create conflicted copy\n    const conflictPath = filePath.replace(\n      /(\\.\\w+)$/,\n      ` (conflicted copy ${new Date().toISOString()})$1`\n    );\n    return { type: 'conflict', conflictPath };\n  }\n}",
          "timeComplexity": "File upload: O(n/c) where n is file size, c is chunk size. Sync: O(k) where k is changed chunks. Dedup check: O(1) hash lookup.",
          "spaceComplexity": "O(u * f) where u is users, f is unique file chunks (deduplication saves ~50%). Metadata: O(u * n) where n is files per user.",
          "keyInsights": "Use content-defined chunking (CDC) instead of fixed-size for better deduplication (Rabin fingerprinting). Implement delta sync - only upload changed blocks. Use exponential backoff for retries. Implement bandwidth throttling. Use compression before upload (gzip). Store multiple versions (keep last 30 days). Use write-ahead log for crash recovery. Implement optimistic locking for conflict detection. Use bloom filters to quickly check if chunks exist. Implement LAN sync (sync locally before cloud). Use notification service with exponential backoff polling. Implement selective sync (sync only chosen folders). Use encryption at rest and in transit. Implement rate limiting per user.",
          "link": "https://dropbox.tech/"
        },
        {
          "id": "hld_10",
          "title": "Design Google Maps",
          "routeName": "google_maps",
          "difficulty": "Hard",
          "companies": ["Google", "Apple", "Uber", "Amazon", "Microsoft"],
          "tags": [
            "System Design",
            "Geospatial",
            "Routing",
            "Graph Algorithms"
          ],
          "description": "Design a mapping and navigation service like Google Maps with location search, routing, real-time traffic, and ETA predictions.",
          "detailedDescription": "Google Maps provides location search, turn-by-turn navigation, real-time traffic updates, ETA predictions, Street View, and business listings. System must handle billions of queries daily, store petabytes of map data, compute optimal routes in real-time considering traffic, and update maps continuously. Key challenges: graph storage and search, routing algorithms at scale, real-time traffic integration, and map data updates.",
          "approach": "1. Graph database for road network (Neo4j or custom). 2. Dijkstra/A* with heuristics for routing. 3. Precompute routes between major junctions. 4. Tile-based map rendering with zoom levels. 5. Real-time traffic from probe data (mobile devices). 6. Machine learning for ETA prediction. 7. Geospatial indexing (S2 cells) for POI search. 8. CDN for map tiles. 9. Contraction hierarchies for faster routing.",
          "solution": "Components: 1) Map Data Storage: road network as graph (nodes=intersections, edges=roads with distance, speed limit, traffic weight). 2) Routing Service: implements A* algorithm with traffic-aware weights, uses bidirectional search. 3) Traffic Service: collects GPS data from phones, aggregates to road segments, computes speed. 4) Tile Service: serves pre-rendered map tiles at 20+ zoom levels from CDN. 5) Search Service: geocoding (address -> coordinates) and reverse geocoding, POI search using inverted index. 6) ETA Prediction: ML model trained on historical trips considering time, day, weather, events. 7) Map Updates: crowd-sourced corrections, satellite imagery analysis, business updates. Graph optimization: use contraction hierarchies (precompute shortcuts), partition graph by regions.",
          "code": "// A* routing algorithm with traffic\nclass RouteCalculator {\n  constructor(graph) {\n    this.graph = graph; // adjacency list with weights\n  }\n  \n  findRoute(start, end, departureTime) {\n    const openSet = new PriorityQueue();\n    const cameFrom = new Map();\n    const gScore = new Map(); // cost from start\n    const fScore = new Map(); // gScore + heuristic\n    \n    gScore.set(start, 0);\n    fScore.set(start, this.heuristic(start, end));\n    openSet.enqueue(start, fScore.get(start));\n    \n    while (!openSet.isEmpty()) {\n      const current = openSet.dequeue();\n      \n      if (current === end) {\n        return this.reconstructPath(cameFrom, current);\n      }\n      \n      for (const neighbor of this.graph.neighbors(current)) {\n        // Get edge weight considering current traffic\n        const edgeWeight = this.getTrafficAwareWeight(\n          current,\n          neighbor,\n          departureTime + (gScore.get(current) || 0)\n        );\n        \n        const tentativeGScore = (gScore.get(current) || Infinity) + edgeWeight;\n        \n        if (tentativeGScore < (gScore.get(neighbor) || Infinity)) {\n          cameFrom.set(neighbor, current);\n          gScore.set(neighbor, tentativeGScore);\n          fScore.set(neighbor, tentativeGScore + this.heuristic(neighbor, end));\n          \n          if (!openSet.contains(neighbor)) {\n            openSet.enqueue(neighbor, fScore.get(neighbor));\n          }\n        }\n      }\n    }\n    \n    return null; // No path found\n  }\n  \n  heuristic(node1, node2) {\n    // Haversine distance / average speed\n    const distance = haversineDistance(\n      node1.lat, node1.lng,\n      node2.lat, node2.lng\n    );\n    return distance / 25; // assume 25 m/s average speed\n  }\n  \n  getTrafficAwareWeight(from, to, time) {\n    const edge = this.graph.getEdge(from, to);\n    const baseTime = edge.distance / edge.speedLimit;\n    \n    // Get current traffic multiplier\n    const trafficData = getTrafficData(edge.id, time);\n    const trafficMultiplier = trafficData ? trafficData.congestion : 1.0;\n    \n    return baseTime * trafficMultiplier;\n  }\n  \n  reconstructPath(cameFrom, current) {\n    const path = [current];\n    while (cameFrom.has(current)) {\n      current = cameFrom.get(current);\n      path.unshift(current);\n    }\n    return path;\n  }\n}\n\n// Traffic data aggregation\nclass TrafficAggregator {\n  async processProbeData(probeData) {\n    // probeData: [{lat, lng, speed, timestamp, heading}]\n    \n    // 1. Map-match to road segments\n    const matchedSegments = await this.mapMatch(probeData);\n    \n    // 2. Aggregate speeds per segment\n    const segmentSpeeds = new Map();\n    for (const point of matchedSegments) {\n      if (!segmentSpeeds.has(point.segmentId)) {\n        segmentSpeeds.set(point.segmentId, []);\n      }\n      segmentSpeeds.get(point.segmentId).push(point.speed);\n    }\n    \n    // 3. Compute average speed and congestion level\n    const trafficUpdate = [];\n    for (const [segmentId, speeds] of segmentSpeeds) {\n      const avgSpeed = speeds.reduce((a, b) => a + b, 0) / speeds.length;\n      const segment = await this.graph.getSegment(segmentId);\n      const congestionRatio = avgSpeed / segment.speedLimit;\n      \n      let congestion;\n      if (congestionRatio > 0.8) congestion = 1.0; // free flow\n      else if (congestionRatio > 0.5) congestion = 1.5; // moderate\n      else if (congestionRatio > 0.3) congestion = 2.0; // heavy\n      else congestion = 3.0; // severe\n      \n      trafficUpdate.push({ segmentId, congestion, avgSpeed });\n    }\n    \n    // 4. Update traffic database\n    await this.updateTrafficDB(trafficUpdate);\n  }\n}",
          "timeComplexity": "A* routing: O(b^d) worst case, O(E log V) typical with priority queue. Traffic aggregation: O(n) for n probe points per batch.",
          "spaceComplexity": "O(V + E) for graph where V is intersections, E is road segments. Map tiles: O(t) where t is tiles at all zoom levels.",
          "keyInsights": "Use contraction hierarchies to precompute shortcuts between important nodes (reduces search space 1000x). Partition graph geographically and compute routes hierarchically (intra-region, then inter-region). Use bidirectional A* (search from both start and end). Cache popular routes. Update traffic every 2-5 minutes. Use probe data from 1%+ of smartphones for coverage. Implement map-matching to snap GPS to roads (Hidden Markov Model). Use vector tiles for efficient transmission. Pre-render tiles at multiple zoom levels. Use quadtree for spatial indexing of POIs. Implement search ranking by relevance, distance, popularity. Store historical traffic patterns for better prediction. Use machine learning for ETA (considers time of day, day of week, weather, events).",
          "link": "https://blog.google/products/maps/"
        }
      ]
    },
    {
      "id": "lld-1",
      "title": "LLD - Data Structures & Caching",
      "summary": "Low-level designs focusing on data structure implementations and caching mechanisms",
      "questions": [
        {
          "id": "lld_1",
          "title": "Design LRU Cache",
          "routeName": "lru_cache",
          "difficulty": "Medium",
          "companies": [
            "Google",
            "Amazon",
            "Microsoft",
            "Apple",
            "Meta",
            "Bloomberg"
          ],
          "tags": ["Data Structures", "Hash Table", "Linked List", "Design"],
          "description": "Design a Least Recently Used (LRU) cache with O(1) time complexity for get and put operations.",
          "detailedDescription": "Implement an LRU cache that supports get(key) and put(key, value) operations. The cache has a fixed capacity. When the cache reaches capacity, it should invalidate the least recently used item before inserting a new item. Both operations must run in O(1) time.",
          "approach": "Use a combination of hash map and doubly linked list. Hash map provides O(1) lookup, doubly linked list maintains order of access. Most recently used items at head, least recently used at tail. On get: move accessed node to head. On put: if exists, update and move to head; if new and at capacity, remove tail node; add new node to head.",
          "solution": "Data structures: 1) HashMap<key, Node*> for O(1) lookup. 2) Doubly linked list with head and tail sentinels. Node structure: {key, value, prev, next}. Operations: get(key) - if exists in map, move node to head, return value; else return -1. put(key, value) - if exists, update value and move to head; if new, create node, add to head, add to map; if size > capacity, remove tail node from list and map. Helper methods: addToHead(node), removeNode(node), moveToHead(node), removeTail().",
          "code": "class LRUCache {\n  constructor(capacity) {\n    this.capacity = capacity;\n    this.cache = new Map();\n    // Sentinel nodes for easier edge case handling\n    this.head = { key: 0, value: 0, prev: null, next: null };\n    this.tail = { key: 0, value: 0, prev: null, next: null };\n    this.head.next = this.tail;\n    this.tail.prev = this.head;\n  }\n  \n  get(key) {\n    if (!this.cache.has(key)) return -1;\n    \n    const node = this.cache.get(key);\n    this.moveToHead(node);\n    return node.value;\n  }\n  \n  put(key, value) {\n    if (this.cache.has(key)) {\n      const node = this.cache.get(key);\n      node.value = value;\n      this.moveToHead(node);\n    } else {\n      const newNode = { key, value, prev: null, next: null };\n      this.cache.set(key, newNode);\n      this.addToHead(newNode);\n      \n      if (this.cache.size > this.capacity) {\n        const removed = this.removeTail();\n        this.cache.delete(removed.key);\n      }\n    }\n  }\n  \n  addToHead(node) {\n    node.prev = this.head;\n    node.next = this.head.next;\n    this.head.next.prev = node;\n    this.head.next = node;\n  }\n  \n  removeNode(node) {\n    node.prev.next = node.next;\n    node.next.prev = node.prev;\n  }\n  \n  moveToHead(node) {\n    this.removeNode(node);\n    this.addToHead(node);\n  }\n  \n  removeTail() {\n    const node = this.tail.prev;\n    this.removeNode(node);\n    return node;\n  }\n}",
          "timeComplexity": "O(1) for both get and put operations",
          "spaceComplexity": "O(capacity) for storing up to capacity key-value pairs",
          "keyInsights": "Doubly linked list allows O(1) removal from middle. Sentinel nodes eliminate null checks. HashMap enables O(1) node access. Order maintenance is key - most recent at head, least recent at tail. Always update order on access. Variant: LFU cache uses frequency count instead of recency.",
          "link": "https://leetcode.com/problems/lru-cache/"
        },
        {
          "id": "lld_3",
          "title": "Design Min Stack",
          "routeName": "min_stack",
          "difficulty": "Medium",
          "companies": ["Amazon", "Google", "Microsoft", "Bloomberg", "Apple"],
          "tags": ["Data Structures", "Stack", "Design"],
          "description": "Design a stack that supports push, pop, top, and retrieving the minimum element in O(1) time.",
          "detailedDescription": "Implement a stack data structure that supports standard operations (push, pop, top) plus getMin() which returns the minimum element in the stack. All operations must run in O(1) time complexity.",
          "approach": "Use two stacks: 1) Main stack for all elements. 2) Min stack that keeps track of minimums. When pushing, always push to main stack; push to min stack only if value <= current min. When popping, pop from both stacks if popped value equals current min. getMin() returns top of min stack.",
          "solution": "Maintain two parallel stacks. Alternative approach: use single stack where each node stores (value, currentMin). Space tradeoff: two stacks use less space on average if duplicates are rare. Implementation details: handle empty stack cases, ensure min stack always has at least one element when main stack is non-empty.",
          "code": "class MinStack {\n  constructor() {\n    this.stack = [];\n    this.minStack = [];\n  }\n  \n  push(val) {\n    this.stack.push(val);\n    \n    // Push to minStack if it's empty or val is new minimum\n    if (this.minStack.length === 0 || val <= this.getMin()) {\n      this.minStack.push(val);\n    }\n  }\n  \n  pop() {\n    if (this.stack.length === 0) return;\n    \n    const val = this.stack.pop();\n    \n    // If popped value was the minimum, remove from minStack\n    if (val === this.getMin()) {\n      this.minStack.pop();\n    }\n    \n    return val;\n  }\n  \n  top() {\n    return this.stack[this.stack.length - 1];\n  }\n  \n  getMin() {\n    return this.minStack[this.minStack.length - 1];\n  }\n}\n\n// Alternative: Single stack storing pairs\nclass MinStackAlt {\n  constructor() {\n    this.stack = []; // stores [value, currentMin]\n  }\n  \n  push(val) {\n    const currentMin = this.stack.length === 0 \n      ? val \n      : Math.min(val, this.getMin());\n    this.stack.push([val, currentMin]);\n  }\n  \n  pop() {\n    return this.stack.pop()?.[0];\n  }\n  \n  top() {\n    return this.stack[this.stack.length - 1]?.[0];\n  }\n  \n  getMin() {\n    return this.stack[this.stack.length - 1]?.[1];\n  }\n}",
          "timeComplexity": "O(1) for all operations - push, pop, top, getMin",
          "spaceComplexity": "O(n) where n is number of elements. Two-stack approach uses O(n) worst case, better on average if few unique minimums.",
          "keyInsights": "Key insight: track minimum at each level of stack. Two-stack approach saves space when minimums don't change frequently. Single-stack approach is simpler but uses more space. Handle edge case: duplicate minimums (use <= not < when comparing). Extension: design MaxStack using same principle. Can combine for MinMaxStack tracking both.",
          "link": "https://leetcode.com/problems/min-stack/"
        },
        {
          "id": "lld_4",
          "title": "Design HashMap",
          "routeName": "hashmap",
          "difficulty": "Medium",
          "companies": ["Google", "Amazon", "Microsoft", "Meta", "Apple"],
          "tags": ["Data Structures", "Hash Table", "Design"],
          "description": "Design a HashMap from scratch with put, get, and remove operations.",
          "detailedDescription": "Implement a HashMap without using built-in hash table libraries. Handle collisions using chaining (linked list) or open addressing. Support put(key, value), get(key), and remove(key) operations. Implement dynamic resizing when load factor exceeds threshold.",
          "approach": "Use array of buckets (linked lists for chaining). Hash function: convert key to array index. Collision handling: chaining with linked lists in each bucket. Load factor: n/capacity. When load factor > 0.75, double capacity and rehash all entries. Hash function should distribute keys uniformly.",
          "solution": "Structure: array of size n (initially 16), each bucket is linked list of (key, value) nodes. put(key, value): compute hash, find bucket, traverse list - if key exists update value, else append new node, check load factor and resize if needed. get(key): compute hash, find bucket, traverse list for key. remove(key): compute hash, find bucket, remove node from list. resize(): create new array of 2x size, rehash and insert all existing entries.",
          "code": "class HashMap {\n  constructor(initialCapacity = 16) {\n    this.capacity = initialCapacity;\n    this.size = 0;\n    this.buckets = new Array(this.capacity).fill(null).map(() => []);\n    this.loadFactorThreshold = 0.75;\n  }\n  \n  hash(key) {\n    let hash = 0;\n    const str = String(key);\n    for (let i = 0; i < str.length; i++) {\n      hash = ((hash << 5) - hash) + str.charCodeAt(i);\n      hash = hash & hash; // Convert to 32-bit integer\n    }\n    return Math.abs(hash) % this.capacity;\n  }\n  \n  put(key, value) {\n    const index = this.hash(key);\n    const bucket = this.buckets[index];\n    \n    // Check if key exists and update\n    for (let i = 0; i < bucket.length; i++) {\n      if (bucket[i].key === key) {\n        bucket[i].value = value;\n        return;\n      }\n    }\n    \n    // Add new entry\n    bucket.push({ key, value });\n    this.size++;\n    \n    // Check load factor and resize if needed\n    if (this.size / this.capacity > this.loadFactorThreshold) {\n      this.resize();\n    }\n  }\n  \n  get(key) {\n    const index = this.hash(key);\n    const bucket = this.buckets[index];\n    \n    for (const entry of bucket) {\n      if (entry.key === key) {\n        return entry.value;\n      }\n    }\n    \n    return undefined;\n  }\n  \n  remove(key) {\n    const index = this.hash(key);\n    const bucket = this.buckets[index];\n    \n    for (let i = 0; i < bucket.length; i++) {\n      if (bucket[i].key === key) {\n        bucket.splice(i, 1);\n        this.size--;\n        return true;\n      }\n    }\n    \n    return false;\n  }\n  \n  resize() {\n    const oldBuckets = this.buckets;\n    this.capacity *= 2;\n    this.buckets = new Array(this.capacity).fill(null).map(() => []);\n    this.size = 0;\n    \n    // Rehash all entries\n    for (const bucket of oldBuckets) {\n      for (const entry of bucket) {\n        this.put(entry.key, entry.value);\n      }\n    }\n  }\n  \n  has(key) {\n    return this.get(key) !== undefined;\n  }\n  \n  keys() {\n    const allKeys = [];\n    for (const bucket of this.buckets) {\n      for (const entry of bucket) {\n        allKeys.push(entry.key);\n      }\n    }\n    return allKeys;\n  }\n  \n  values() {\n    const allValues = [];\n    for (const bucket of this.buckets) {\n      for (const entry of bucket) {\n        allValues.push(entry.value);\n      }\n    }\n    return allValues;\n  }\n}",
          "timeComplexity": "O(1) average case for put/get/remove. O(n) worst case if all keys collide. Resize is O(n) but amortized O(1).",
          "spaceComplexity": "O(n) where n is number of key-value pairs stored",
          "keyInsights": "Good hash function is critical for performance. Load factor balances space vs time. Common threshold: 0.75. Chaining handles collisions simply. Alternative: open addressing (linear probing, quadratic probing, double hashing). Resize by doubling ensures amortized O(1) insertions. Consider using prime numbers for capacity to reduce collisions. Handle hash collisions gracefully. For production: use cryptographic hash for string keys to prevent DoS attacks.",
          "link": "https://leetcode.com/problems/design-hashmap/"
        },
        {
          "id": "lld_5",
          "title": "Design Circular Queue",
          "routeName": "circular_queue",
          "difficulty": "Medium",
          "companies": ["Amazon", "Microsoft", "Google", "Bloomberg"],
          "tags": ["Data Structures", "Queue", "Array", "Design"],
          "description": "Design a circular queue (ring buffer) with fixed size supporting enQueue, deQueue, Front, Rear, isEmpty, and isFull operations.",
          "detailedDescription": "Implement a circular queue using a fixed-size array. The queue wraps around when reaching the end of the array. Support enQueue (add element), deQueue (remove element), Front (get front element), Rear (get last element), isEmpty, and isFull checks. Handle full and empty conditions correctly.",
          "approach": "Use array of fixed size with two pointers: front and rear. Initialize front and rear to -1. Use modulo arithmetic to wrap around. Track size or use clever pointer arithmetic to distinguish full from empty. enQueue: increment rear (with wrap), add element. deQueue: increment front (with wrap), return element. Handle edge cases: first element, last element, full queue, empty queue.",
          "solution": "Array with capacity k, front and rear indices, size counter. enQueue: if full return false; if empty set front=0; rear = (rear+1) % capacity; add element; increment size. deQueue: if empty return false; get element at front; if size==1 reset to empty state; else front = (front+1) % capacity; decrement size. isEmpty: size == 0. isFull: size == capacity. Alternative: use (rear+1) % capacity == front to detect full (wastes one slot).",
          "code": "class CircularQueue {\n  constructor(k) {\n    this.capacity = k;\n    this.queue = new Array(k);\n    this.front = -1;\n    this.rear = -1;\n    this.size = 0;\n  }\n  \n  enQueue(value) {\n    if (this.isFull()) return false;\n    \n    // If queue is empty, set front to 0\n    if (this.isEmpty()) {\n      this.front = 0;\n    }\n    \n    // Move rear pointer with wrap-around\n    this.rear = (this.rear + 1) % this.capacity;\n    this.queue[this.rear] = value;\n    this.size++;\n    \n    return true;\n  }\n  \n  deQueue() {\n    if (this.isEmpty()) return false;\n    \n    // If this was the last element, reset queue\n    if (this.size === 1) {\n      this.front = -1;\n      this.rear = -1;\n    } else {\n      // Move front pointer with wrap-around\n      this.front = (this.front + 1) % this.capacity;\n    }\n    \n    this.size--;\n    return true;\n  }\n  \n  Front() {\n    if (this.isEmpty()) return -1;\n    return this.queue[this.front];\n  }\n  \n  Rear() {\n    if (this.isEmpty()) return -1;\n    return this.queue[this.rear];\n  }\n  \n  isEmpty() {\n    return this.size === 0;\n  }\n  \n  isFull() {\n    return this.size === this.capacity;\n  }\n}\n\n// Alternative implementation without size counter\nclass CircularQueueAlt {\n  constructor(k) {\n    this.capacity = k + 1; // One extra space to distinguish full from empty\n    this.queue = new Array(this.capacity);\n    this.front = 0;\n    this.rear = 0;\n  }\n  \n  enQueue(value) {\n    if (this.isFull()) return false;\n    \n    this.queue[this.rear] = value;\n    this.rear = (this.rear + 1) % this.capacity;\n    return true;\n  }\n  \n  deQueue() {\n    if (this.isEmpty()) return false;\n    \n    this.front = (this.front + 1) % this.capacity;\n    return true;\n  }\n  \n  Front() {\n    if (this.isEmpty()) return -1;\n    return this.queue[this.front];\n  }\n  \n  Rear() {\n    if (this.isEmpty()) return -1;\n    const rearIndex = (this.rear - 1 + this.capacity) % this.capacity;\n    return this.queue[rearIndex];\n  }\n  \n  isEmpty() {\n    return this.front === this.rear;\n  }\n  \n  isFull() {\n    return (this.rear + 1) % this.capacity === this.front;\n  }\n}",
          "timeComplexity": "O(1) for all operations - enQueue, deQueue, Front, Rear, isEmpty, isFull",
          "spaceComplexity": "O(k) where k is the capacity of the queue",
          "keyInsights": "Modulo arithmetic enables wrap-around. Two approaches: 1) Use size counter (cleaner, uses all space). 2) Waste one slot to distinguish full from empty (front == rear means empty, (rear+1)%capacity == front means full). Circular queue is more efficient than regular queue with array (no need to shift elements). Used in: producer-consumer problems, streaming data buffers, scheduling algorithms. Handle edge cases carefully: empty to first element, last element to empty.",
          "link": "https://leetcode.com/problems/design-circular-queue/"
        },
        {
          "id": "lld_6",
          "title": "Design Browser History",
          "routeName": "browser_history",
          "difficulty": "Medium",
          "companies": ["Google", "Microsoft", "Amazon", "Apple"],
          "tags": ["Data Structures", "Stack", "Design", "Linked List"],
          "description": "Design a browser history system supporting visit, back, and forward operations.",
          "detailedDescription": "Implement a browser history class that stores visited URLs and supports: visit(url) - visit a new URL (clears forward history), back(steps) - go back steps in history, forward(steps) - go forward steps in history. Handle boundary conditions when steps exceed available history.",
          "approach": "Use doubly linked list or two stacks or array with pointer. Array approach: maintain current index, store URLs in array, visiting new URL truncates array at current position. Doubly linked list: each node is a URL, current pointer, visiting creates new node and clears forward links.",
          "solution": "Array-based implementation: history array, currentIndex. visit(url): truncate array at currentIndex+1, append url, increment currentIndex. back(steps): currentIndex = max(0, currentIndex - steps), return history[currentIndex]. forward(steps): currentIndex = min(history.length-1, currentIndex + steps), return history[currentIndex]. Alternative: use two stacks (backStack, forwardStack) for O(1) back/forward but O(steps) for multi-step operations.",
          "code": "class BrowserHistory {\n  constructor(homepage) {\n    this.history = [homepage];\n    this.currentIndex = 0;\n  }\n  \n  visit(url) {\n    // Truncate forward history\n    this.history = this.history.slice(0, this.currentIndex + 1);\n    this.history.push(url);\n    this.currentIndex++;\n  }\n  \n  back(steps) {\n    this.currentIndex = Math.max(0, this.currentIndex - steps);\n    return this.history[this.currentIndex];\n  }\n  \n  forward(steps) {\n    this.currentIndex = Math.min(\n      this.history.length - 1,\n      this.currentIndex + steps\n    );\n    return this.history[this.currentIndex];\n  }\n}\n\n// Alternative: Doubly Linked List implementation\nclass BrowserHistoryLinkedList {\n  constructor(homepage) {\n    this.current = { url: homepage, prev: null, next: null };\n  }\n  \n  visit(url) {\n    const newNode = { url, prev: this.current, next: null };\n    this.current.next = newNode;\n    this.current = newNode;\n  }\n  \n  back(steps) {\n    while (steps > 0 && this.current.prev) {\n      this.current = this.current.prev;\n      steps--;\n    }\n    return this.current.url;\n  }\n  \n  forward(steps) {\n    while (steps > 0 && this.current.next) {\n      this.current = this.current.next;\n      steps--;\n    }\n    return this.current.url;\n  }\n}\n\n// Alternative: Two Stacks implementation\nclass BrowserHistoryStacks {\n  constructor(homepage) {\n    this.backStack = [];\n    this.forwardStack = [];\n    this.current = homepage;\n  }\n  \n  visit(url) {\n    this.backStack.push(this.current);\n    this.current = url;\n    this.forwardStack = []; // Clear forward history\n  }\n  \n  back(steps) {\n    while (steps > 0 && this.backStack.length > 0) {\n      this.forwardStack.push(this.current);\n      this.current = this.backStack.pop();\n      steps--;\n    }\n    return this.current;\n  }\n  \n  forward(steps) {\n    while (steps > 0 && this.forwardStack.length > 0) {\n      this.backStack.push(this.current);\n      this.current = this.forwardStack.pop();\n      steps--;\n    }\n    return this.current;\n  }\n}",
          "timeComplexity": "Array: O(1) visit, O(1) back/forward. Linked List: O(1) visit, O(steps) back/forward. Stacks: O(1) visit, O(steps) back/forward.",
          "spaceComplexity": "O(n) where n is number of unique URLs visited",
          "keyInsights": "Array approach is simplest and most efficient for typical use. Visiting new URL must clear forward history. Handle boundary conditions: can't go back beyond start, can't go forward beyond end. Linked list saves memory if history is sparse but slower for multi-step navigation. Two stacks approach natural for single-step but inefficient for multi-step. Real browsers optimize with: URL deduplication, memory limits (discard old history), disk persistence, tab isolation.",
          "link": "https://leetcode.com/problems/design-browser-history/"
        }
      ]
    }
  ]
}
